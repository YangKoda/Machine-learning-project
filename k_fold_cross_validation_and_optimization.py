# -*- coding: utf-8 -*-
"""K fold cross validation and Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sBofn5fb6POhLkEwlSreM2z66s9aWq8P
"""

"""
Importing the libraries
"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from time import time
from scipy.stats import randint

start = time()
"""Read data from the file"""
dataset = pd.read_csv('ObesityDataSet.csv')
X = dataset.drop(columns=['NObeyesdad'], axis=1)
y = dataset['NObeyesdad']

"""Check for missing values in each column"""
missing_values = dataset.isnull().sum()
print(missing_values)

"""Displaying dataset"""
# Assuming X and y are numpy arrays
# Convert the numpy array to a pandas DataFrame
print(type(X))
df_X = pd.DataFrame(X)
df_y = pd.Series(y)
# Set display options to show all rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
# Print the DataFrame and Series
print(df_X)

"""Exploratory Data Analysis (EDA)"""
# Scatter plot of all features against the target
column_names = dataset.columns[:-1]
plt.figure(figsize=(20, 15))
for i, col in enumerate(df_X.columns, 1):
    plt.subplot(5, 5, i)
    plt.scatter(df_X[col], df_y, alpha=0.5)
    plt.title(f'{column_names[i-1]} vs Obesity')
    plt.xlabel(column_names[i-1])
    plt.ylabel('Target')
plt.tight_layout()
plt.show()

"""Dropping some features"""
# Define the columns to drop
columns_to_drop = [15]
# Drop the columns from the NumPy array
X_dropped = np.delete(X, columns_to_drop, axis=1)
# Convert the result to a DataFrame if needed
df_X = pd.DataFrame(X_dropped)
# Display the resulting DataFrame
print(df_X)

"""Encoding categorical data with LabelEncoder"""
le = LabelEncoder()
columns_to_encode = ['Gender', 'family_history_with_overweight',
                     'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC'] # MTRANS removed due to moderate correlation 0.6 with age
X = X.drop('MTRANS', axis=1)
X[columns_to_encode] = X[columns_to_encode].apply(le.fit_transform)
y = le.fit_transform(y)
print(X)
print(y)

"""Feature Scaling"""
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
columns_to_scale = [num_col for num_col in dataset.columns if dataset[num_col].dtype != 'object']
X[columns_to_scale] = sc.fit_transform(dataset[columns_to_scale])

"""Splitting the dataset into Training Set and Test set"""
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

"""Hyperparameter tuning"""

param_grid_knn = {
    'n_neighbors': list(range(1, 31)),
}

param_grid_svm = {
    'C': [0.1, 1, 10, 100, 1000],
    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
    'kernel': ['linear', 'rbf']
}

param_grid_decision_tree = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 2, 4, 6, 8, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3, 4, 5]
}

param_grid_random_forest = {
    'n_estimators': [10, 50, 100, 200],
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 2, 4, 6, 8, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3, 4, 5]
}

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=74)
accuracy = make_scorer(accuracy_score)

grid_kNN = GridSearchCV(estimator=KNeighborsClassifier(),
                        param_grid=param_grid_knn, cv=cv, scoring=accuracy)
grid_kNN.fit(X_train, y_train)
print(f'Grid kNN score: {grid_kNN.best_score_}')
print('Best Parameters:', grid_kNN.best_params_)

grid_svm = GridSearchCV(estimator=SVC(),
                        param_grid=param_grid_svm, cv=cv, scoring=accuracy)
grid_svm.fit(X_train, y_train)
print(f'Grid Support vector classifier score: {grid_svm.best_score_}')
print('Best Parameters:', grid_svm.best_params_)

grid_dt = GridSearchCV(estimator=DecisionTreeClassifier(),
                        param_grid=param_grid_decision_tree, cv=cv, scoring=accuracy)
grid_dt.fit(X_train, y_train)
print(f'Grid Decision Tree score: {grid_dt.best_score_}')
print('Best Parameters:', grid_dt.best_params_)

# random_search_rf = GridSearchCV(estimator=RandomForestClassifier(),
#                         param_grid=param_grid_random_forest)
# random_search_rf.fit(X_train, y_train)
# print(f'Grid Random Forest score: {random_search_rf.score(X_train, y_train)}')

# Define the parameter distribution
param_dist = {
    'criterion': ['gini', 'entropy'],
    'n_estimators': randint(10, 200),
    'max_depth': randint(1, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20)
}

# Create a RandomizedSearchCV object
random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist, n_iter=100, cv=cv, scoring=accuracy)
random_search_rf.fit(X_train, y_train)
print(f'Random Forest score: {random_search_rf.best_score_}')
print('Best Parameters:', random_search_rf.best_params_)

"""Training the K-NN model on the training set"""
classifier = KNeighborsClassifier(n_neighbors=grid_kNN.best_params_['n_neighbors'])
classifier.fit(X_train, y_train)

"""Predicting the Test set results"""
y_pred_kNN = classifier.predict(X_test)
# print(np.concatenate((y_pred_kNN.reshape(len(y_pred_kNN),1), y_test.reshape(len(y_test),1)),1))

"""Making the Confusion Matrix"""
cm = confusion_matrix(y_test, y_pred_kNN)
print(cm)
print(f'Accuracy Score of kNN {accuracy_score(y_test, y_pred_kNN)}')

"""Training the Kernel SVM model on the training set"""
classifier = SVC(C=grid_svm.best_params_['C'],
                 gamma=grid_svm.best_params_['gamma'],
                 kernel=grid_svm.best_params_['kernel'])
classifier.fit(X_train, y_train)

"""Predicting the Test set results"""
y_pred_svc = classifier.predict(X_test)
# print(np.concatenate((y_pred_svc.reshape(len(y_pred_svc),1), y_test.reshape(len(y_test),1)),1))

"""Making the Confusion Matrix"""
cm = confusion_matrix(y_test, y_pred_svc)
print(cm)
print(f'Accuracy Score of SVC {accuracy_score(y_test, y_pred_svc)}')

"""Training the Naive Bayes model on the training set"""
classifier = GaussianNB()
classifier.fit(X_train, y_train)

"""Predicting the Test set results"""
y_pred_nb = classifier.predict(X_test)
# print(np.concatenate((y_pred_nb.reshape(len(y_pred_nb),1), y_test.reshape(len(y_test),1)),1))

"""Making the Confusion Matrix"""
cm = confusion_matrix(y_test, y_pred_nb)
print(cm)
print(f'Accuracy Score of Gaussian Naive Bayes {accuracy_score(y_test, y_pred_nb)}')

"""Training the Decision Tree Classification model on the training set"""
classifier = DecisionTreeClassifier(criterion=grid_dt.best_params_['criterion'],
                                    max_depth=grid_dt.best_params_['max_depth'],
                                    min_samples_split=grid_dt.best_params_['min_samples_split'],
                                    min_samples_leaf=grid_dt.best_params_['min_samples_leaf'])
classifier.fit(X_train, y_train)

"""Predicting the Test set results"""
y_pred_dt = classifier.predict(X_test)
# print(np.concatenate((y_pred_dt.reshape(len(y_pred_dt),1), y_test.reshape(len(y_test),1)),1))

"""Making the Confusion Matrix"""
cm = confusion_matrix(y_test, y_pred_dt)
print(cm)
print(f'Accuracy Score of Decision tree {accuracy_score(y_test, y_pred_dt)}')

"""Training the Random Forest Classification model on the training set"""
classifier = RandomForestClassifier(n_estimators=random_search_rf.best_params_['n_estimators'],
                                    criterion=random_search_rf.best_params_['criterion'],
                                    max_depth=random_search_rf.best_params_['max_depth'],
                                    min_samples_split=random_search_rf.best_params_['min_samples_split'],
                                    min_samples_leaf=random_search_rf.best_params_['min_samples_leaf'],
                                    random_state=74)
classifier.fit(X_train, y_train)

"""Predicting the Test set results"""
y_pred_rf = classifier.predict(X_test)
# print(np.concatenate((y_pred_rf.reshape(len(y_pred_rf),1), y_test.reshape(len(y_test),1)),1))

"""Making the Confusion Matrix"""
cm = confusion_matrix(y_test, y_pred_rf)
print(cm)
print(f'Accuracy Score of Random Forest {accuracy_score(y_test, y_pred_rf)}')

end = time()
print(f'Total time taken: {(end-start)/60} minutes')